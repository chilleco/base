x-sentry-env: &sentry_env
  SENTRY_CONF: /etc/sentry
  SENTRY_SECRET_KEY: ${SENTRY_SECRET_KEY:-change-me}
  SENTRY_SINGLE_ORGANIZATION: ${SENTRY_SINGLE_ORGANIZATION:-1}
  SENTRY_RELAY_OPEN_REGISTRATION: ${SENTRY_RELAY_OPEN_REGISTRATION:-true}
  SENTRY_EVENT_RETENTION_DAYS: ${SENTRY_EVENT_RETENTION_DAYS:-30}
  SENTRY_KAFKA_BROKERS: ${SENTRY_KAFKA_BROKERS:-sentry-kafka:9092}
  SNUBA: http://sentry-snuba-api:1218
  SENTRY_POSTGRES_HOST: sentry-postgres
  SENTRY_DB_NAME: ${SENTRY_DB_NAME:-sentry}
  SENTRY_DB_USER: ${SENTRY_DB_USER:-sentry}
  SENTRY_DB_PASSWORD: ${SENTRY_DB_PASS:-sentry}
  SENTRY_REDIS_HOST: sentry-redis
  SENTRY_REDIS_PORT: 6379
  SENTRY_SERVER_EMAIL: ${SENTRY_SERVER_EMAIL:-sentry@localhost}
  SENTRY_EMAIL_HOST: ${SENTRY_SMTP_HOST:-}
  SENTRY_EMAIL_PORT: ${SENTRY_SMTP_PORT:-587}
  SENTRY_EMAIL_USER: ${SENTRY_SMTP_USER:-}
  SENTRY_EMAIL_PASSWORD: ${SENTRY_SMTP_PASS:-}
  SENTRY_EMAIL_USE_TLS: ${SENTRY_SMTP_USE_TLS:-true}

x-snuba-env: &snuba_env
  SNUBA_SETTINGS: self_hosted
  CLICKHOUSE_HOST: sentry-clickhouse
  DEFAULT_BROKERS: ${SENTRY_KAFKA_BROKERS:-sentry-kafka:9092}
  REDIS_HOST: sentry-redis
  UWSGI_MAX_REQUESTS: "10000"
  UWSGI_DISABLE_LOGGING: "true"
  SENTRY_EVENT_RETENTION_DAYS: ${SENTRY_EVENT_RETENTION_DAYS:-30}

x-sentry-volumes: &sentry_volumes
  - ${DATA_PATH}/sentry/files:/var/lib/sentry/files
  - ./infra/sentry/sentry.conf.py:/etc/sentry/sentry.conf.py:ro

services:
  s3:
    image: minio/minio:latest
    container_name: base-s3
    volumes:
      - ${DATA_PATH}/s3:/data
    environment:
      MINIO_ROOT_USER: ${MINIO_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_PASS}
    ports:
      - 9000:9000
      - 9001:9001
    command: server /data --console-address ":9001"

  mongo:
    image: mongo:6.0
    container_name: base-mongo
    ports:
      - 27017:27017
    restart: unless-stopped
    environment:
      MONGO_INITDB_DATABASE: admin
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASS}
    volumes:
      - ${DATA_PATH}/mongo:/data/db
      - ./infra/mongo/mongod.conf:/etc/mongod.conf
    command: --config /etc/mongod.conf

  # redis:
  #   image: redis:7.0
  #   restart: unless-stopped
  #   env_file:
  #     - .env
  #   volumes:
  #     - ${DATA_PATH}/redis:/data
  #   ports:
  #     - ${REDIS_PORT}:6379
  #   command: bash -c "redis-server --requirepass ${REDIS_PASS}"

  node_exporter:
    image: quay.io/prometheus/node-exporter:v1.10.2
    container_name: base-node_exporter
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
      - ./infra/prometheus/textfile:/textfile
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--path.rootfs=/rootfs"
      - "--collector.cpufreq"
      - "--collector.textfile.directory=/textfile"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc|rootfs)($$|/)"

  node_exporter_textfile:
    image: busybox:1.37.0
    container_name: base-node_exporter_textfile
    restart: unless-stopped
    volumes:
      - /proc:/host/proc:ro
      - ./infra/prometheus/textfile:/textfile
    command: >
      sh -c 'while true; do
      hz=$$(awk -F": " "/MHz/ {sum+=\$$2; cnt++} END {if (cnt>0) printf \"%.0f\", (sum/cnt)*1000000; else print 0}" /host/proc/cpuinfo);
      printf "cpu_frequency_hz %s\n" "$$hz" > /textfile/cpu_frequency.prom;
      sleep 15;
      done'

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.52.1
    container_name: base-cadvisor
    restart: unless-stopped
    privileged: true
    pid: "host"
    volumes:
      - /:/rootfs:ro
      - /sys:/sys:ro
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /dev/disk:/dev/disk:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro

  prometheus:
    image: prom/prometheus:v3.2.1
    container_name: base-prometheus
    restart: unless-stopped
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./infra/prometheus/rules:/etc/prometheus/rules:ro
      - ${DATA_PATH}/prometheus:/prometheus
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.external-url=${PROTOCOL}://${EXTERNAL_HOST}/prometheus/"
      - "--web.route-prefix=/"
      - "--web.enable-lifecycle"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    ports:
      - "${PROMETHEUS_PORT:-9090}:9090"
    depends_on:
      - node_exporter
      - cadvisor
      # - postgres-exporter
      # - redis-exporter

  # ==========================================================================
  # LOKI - Log Aggregation
  # ==========================================================================
  loki:
    image: grafana/loki:3.6.4
    container_name: base-loki
    restart: unless-stopped
    command: -config.file=/etc/loki/loki-config.yml
    volumes:
      - ./infra/loki/loki-config.yml:/etc/loki/loki-config.yml:ro
      - ${DATA_PATH}/loki:/loki
    ports:
      - "${LOKI_PORT:-3100}:3100"
    # NOTE: Loki 3.x distroless image has no shell or http tools for healthcheck
    healthcheck:
      disable: true

  # ==========================================================================
  # ALLOY - Unified Log Collector (replaces Promtail)
  # ==========================================================================
  alloy:
    image: grafana/alloy:v1.12.2
    container_name: base-alloy
    restart: unless-stopped
    command:
      - run
      - /etc/alloy/config.alloy
      - --storage.path=/var/lib/alloy/data
      - --server.http.listen-addr=0.0.0.0:12345
      - --stability.level=generally-available
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./infra/alloy/config.alloy:/etc/alloy/config.alloy:ro
      - ${DATA_PATH}/alloy:/var/lib/alloy/data
    ports:
      - "${ALLOY_PORT:-12345}:12345"
    depends_on:
      loki:
        condition: service_started

  grafana:
    image: grafana/grafana:12.3.1
    container_name: base-grafana
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASS:-admin}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: "${PROTOCOL}://${EXTERNAL_HOST}/grafana/"
      # GF_SERVER_SERVE_FROM_SUB_PATH: "true"
      # # Alerting - Telegram notifications (webhook to thread/topic)
      # GF_UNIFIED_ALERTING_ENABLED: "true"
      # TELEGRAM_ALERT_BOT_TOKEN: ${TELEGRAM_ALERT_BOT_TOKEN:-}
      # TELEGRAM_ALERT_CHAT_ID: ${TELEGRAM_ALERT_CHAT_ID:-}
      # TELEGRAM_ALERT_THREAD_ID: ${TELEGRAM_ALERT_THREAD_ID:-0}
    volumes:
      - ${DATA_PATH}/grafana:/var/lib/grafana
      - ./infra/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./infra/grafana/dashboards:/etc/grafana/dashboards:ro
    ports:
      - "${GRAFANA_PORT:-3000}:3000"
    depends_on:
      - prometheus
      - loki

  # # ==========================================================================
  # # SENTRY - Error aggregation, triage UI and alerting
  # # ==========================================================================
  # sentry-redis:
  #   image: redis:7.4-alpine
  #   container_name: base-sentry-redis
  #   restart: unless-stopped
  #   volumes:
  #     - ${DATA_PATH}/sentry/redis:/data
  #   command: redis-server --save 60 1 --loglevel warning
  #   healthcheck:
  #     test: ["CMD", "redis-cli", "ping"]
  #     interval: 15s
  #     timeout: 5s
  #     retries: 10

  # sentry-postgres:
  #   image: postgres:16-alpine
  #   container_name: base-sentry-postgres
  #   restart: unless-stopped
  #   environment:
  #     POSTGRES_DB: ${SENTRY_DB_NAME:-sentry}
  #     POSTGRES_USER: ${SENTRY_DB_USER:-sentry}
  #     POSTGRES_PASSWORD: ${SENTRY_DB_PASS:-sentry}
  #   volumes:
  #     - ${DATA_PATH}/sentry/postgres:/var/lib/postgresql/data
  #   healthcheck:
  #     test: ["CMD-SHELL", "pg_isready -U ${SENTRY_DB_USER:-sentry} -d ${SENTRY_DB_NAME:-sentry}"]
  #     interval: 15s
  #     timeout: 5s
  #     retries: 10

  # sentry-kafka:
  #   image: confluentinc/cp-kafka:7.6.6
  #   container_name: base-sentry-kafka
  #   restart: unless-stopped
  #   environment:
  #     KAFKA_PROCESS_ROLES: "broker,controller"
  #     KAFKA_CONTROLLER_QUORUM_VOTERS: "1001@127.0.0.1:29093"
  #     KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"
  #     KAFKA_NODE_ID: "1001"
  #     CLUSTER_ID: "MkU3OEVBNTcwNTJENDM2Qk"
  #     KAFKA_LISTENERS: "PLAINTEXT://0.0.0.0:29092,INTERNAL://0.0.0.0:9093,EXTERNAL://0.0.0.0:9092,CONTROLLER://0.0.0.0:29093"
  #     KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://127.0.0.1:29092,INTERNAL://sentry-kafka:9093,EXTERNAL://sentry-kafka:9092"
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "PLAINTEXT:PLAINTEXT,INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT"
  #     KAFKA_INTER_BROKER_LISTENER_NAME: "PLAINTEXT"
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "1"
  #     KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS: "1"
  #     KAFKA_LOG_RETENTION_HOURS: "24"
  #     KAFKA_MESSAGE_MAX_BYTES: "50000000"
  #     KAFKA_MAX_REQUEST_SIZE: "50000000"
  #     CONFLUENT_SUPPORT_METRICS_ENABLE: "false"
  #   volumes:
  #     - ${DATA_PATH}/sentry/kafka:/var/lib/kafka/data
  #   healthcheck:
  #     test: ["CMD-SHELL", "nc -z localhost 9092"]
  #     interval: 15s
  #     timeout: 10s
  #     retries: 20

  # sentry-clickhouse:
  #   image: altinity/clickhouse-server:25.3.6.10034.altinitystable
  #   container_name: base-sentry-clickhouse
  #   restart: unless-stopped
  #   ulimits:
  #     nofile:
  #       soft: 262144
  #       hard: 262144
  #   volumes:
  #     - ${DATA_PATH}/sentry/clickhouse:/var/lib/clickhouse
  #     - ${DATA_PATH}/sentry/clickhouse-log:/var/log/clickhouse-server
  #   healthcheck:
  #     test: ["CMD-SHELL", "HTTP_PROXY='' http_proxy='' wget -nv -t1 --spider 'http://localhost:8123/' || exit 1"]
  #     interval: 15s
  #     timeout: 10s
  #     retries: 20

  # sentry-snuba-bootstrap:
  #   image: getsentry/snuba:latest
  #   container_name: base-sentry-snuba-bootstrap
  #   restart: "no"
  #   environment: *snuba_env
  #   command: bootstrap --force
  #   depends_on:
  #     sentry-clickhouse:
  #       condition: service_healthy
  #     sentry-kafka:
  #       condition: service_healthy
  #     sentry-redis:
  #       condition: service_healthy

  # sentry-snuba-api:
  #   image: getsentry/snuba:latest
  #   container_name: base-sentry-snuba-api
  #   restart: unless-stopped
  #   environment: *snuba_env
  #   command: api
  #   depends_on:
  #     sentry-snuba-bootstrap:
  #       condition: service_completed_successfully
  #   healthcheck:
  #     test: ["CMD", "/bin/bash", "-c", "exec 3<>/dev/tcp/127.0.0.1/1218 && echo -e \"GET /health HTTP/1.1\\r\\nhost: 127.0.0.1\\r\\n\\r\\n\" >&3 && grep ok -s -m 1 <&3"]
  #     interval: 15s
  #     timeout: 10s
  #     retries: 20

  # sentry-snuba-errors-consumer:
  #   image: getsentry/snuba:latest
  #   container_name: base-sentry-snuba-errors-consumer
  #   restart: unless-stopped
  #   environment: *snuba_env
  #   command: rust-consumer --storage errors --consumer-group snuba-consumers --auto-offset-reset=latest --max-batch-time-ms 750 --no-strict-offset-reset
  #   depends_on:
  #     sentry-snuba-bootstrap:
  #       condition: service_completed_successfully

  # sentry-snuba-outcomes-consumer:
  #   image: getsentry/snuba:latest
  #   container_name: base-sentry-snuba-outcomes-consumer
  #   restart: unless-stopped
  #   environment: *snuba_env
  #   command: rust-consumer --storage outcomes_raw --consumer-group snuba-consumers --auto-offset-reset=earliest --max-batch-time-ms 750 --no-strict-offset-reset
  #   depends_on:
  #     sentry-snuba-bootstrap:
  #       condition: service_completed_successfully

  # sentry-snuba-group-attributes-consumer:
  #   image: getsentry/snuba:latest
  #   container_name: base-sentry-snuba-group-attributes-consumer
  #   restart: unless-stopped
  #   environment: *snuba_env
  #   command: rust-consumer --storage group_attributes --consumer-group snuba-group-attributes-consumers --auto-offset-reset=latest --max-batch-time-ms 750 --no-strict-offset-reset
  #   depends_on:
  #     sentry-snuba-bootstrap:
  #       condition: service_completed_successfully

  # sentry-snuba-subscription-consumer-events:
  #   image: getsentry/snuba:latest
  #   container_name: base-sentry-snuba-subscription-consumer-events
  #   restart: unless-stopped
  #   environment: *snuba_env
  #   command: subscriptions-scheduler-executor --dataset events --entity events --auto-offset-reset=latest --no-strict-offset-reset --consumer-group=snuba-events-subscriptions-consumers --followed-consumer-group=snuba-consumers --schedule-ttl=60 --stale-threshold-seconds=900
  #   depends_on:
  #     sentry-snuba-bootstrap:
  #       condition: service_completed_successfully

  # sentry-snuba-replacer:
  #   image: getsentry/snuba:latest
  #   container_name: base-sentry-snuba-replacer
  #   restart: unless-stopped
  #   environment: *snuba_env
  #   command: replacer --storage errors --auto-offset-reset=latest --no-strict-offset-reset
  #   depends_on:
  #     sentry-snuba-bootstrap:
  #       condition: service_completed_successfully

  # sentry-web:
  #   image: getsentry/sentry:latest
  #   container_name: base-sentry-web
  #   restart: unless-stopped
  #   environment: *sentry_env
  #   volumes: *sentry_volumes
  #   command: sh -c "sentry upgrade --noinput && sentry run web"
  #   ports:
  #     - "${SENTRY_PORT:-9002}:9000"
  #   depends_on:
  #     sentry-postgres:
  #       condition: service_healthy
  #     sentry-redis:
  #       condition: service_healthy
  #     sentry-kafka:
  #       condition: service_healthy
  #     sentry-snuba-api:
  #       condition: service_healthy

  # sentry-relay:
  #   image: getsentry/relay:latest
  #   container_name: base-sentry-relay
  #   restart: unless-stopped
  #   volumes:
  #     - ./infra/sentry/relay:/work/.relay
  #   command: run --config /work/.relay
  #   ports:
  #     - "${SENTRY_RELAY_PORT:-9003}:3000"
  #   healthcheck:
  #     test: ["CMD", "/bin/relay", "healthcheck"]
  #     interval: 15s
  #     timeout: 5s
  #     retries: 10
  #   depends_on:
  #     sentry-web:
  #       condition: service_started

  # sentry-worker:
  #   image: getsentry/sentry:latest
  #   container_name: base-sentry-worker
  #   restart: unless-stopped
  #   environment: *sentry_env
  #   volumes: *sentry_volumes
  #   command: run worker
  #   depends_on:
  #     sentry-web:
  #       condition: service_started

  # sentry-scheduler:
  #   image: getsentry/sentry:latest
  #   container_name: base-sentry-scheduler
  #   restart: unless-stopped
  #   environment: *sentry_env
  #   volumes: *sentry_volumes
  #   command: run cron
  #   depends_on:
  #     sentry-web:
  #       condition: service_started

  # sentry-events-consumer:
  #   image: getsentry/sentry:latest
  #   container_name: base-sentry-events-consumer
  #   restart: unless-stopped
  #   environment: *sentry_env
  #   volumes: *sentry_volumes
  #   command: run consumer ingest-events --consumer-group ingest-consumer
  #   depends_on:
  #     sentry-web:
  #       condition: service_started

  # sentry-attachments-consumer:
  #   image: getsentry/sentry:latest
  #   container_name: base-sentry-attachments-consumer
  #   restart: unless-stopped
  #   environment: *sentry_env
  #   volumes: *sentry_volumes
  #   command: run consumer ingest-attachments --consumer-group ingest-consumer
  #   depends_on:
  #     sentry-web:
  #       condition: service_started

  # sentry-post-process-forwarder-errors:
  #   image: getsentry/sentry:latest
  #   container_name: base-sentry-post-process-forwarder-errors
  #   restart: unless-stopped
  #   environment: *sentry_env
  #   volumes: *sentry_volumes
  #   command: run consumer --no-strict-offset-reset post-process-forwarder-errors --consumer-group post-process-forwarder --synchronize-commit-log-topic=snuba-commit-log --synchronize-commit-group=snuba-consumers
  #   depends_on:
  #     sentry-web:
  #       condition: service_started

  # sentry-subscription-consumer-events:
  #   image: getsentry/sentry:latest
  #   container_name: base-sentry-subscription-consumer-events
  #   restart: unless-stopped
  #   environment: *sentry_env
  #   volumes: *sentry_volumes
  #   command: run consumer events-subscription-results --consumer-group query-subscription-consumer
  #   depends_on:
  #     sentry-web:
  #       condition: service_started
